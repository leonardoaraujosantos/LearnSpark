{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset (RDD) Tutorial\n",
    "\n",
    "#### References:\n",
    "* https://github.com/holdenk/learning-spark-examples\n",
    "* https://www.youtube.com/watch?v=pZQsDloGB4w\n",
    "* https://spark.apache.org/docs/latest/programming-guide.html\n",
    "* https://www.youtube.com/watch?v=ZojIGRS3HLY\n",
    "* https://www.youtube.com/watch?v=bJouNc1REno\n",
    "* https://www.youtube.com/watch?v=vtxwXSGl9V8\n",
    "* https://www.youtube.com/watch?v=Cn4xdiCxxtw\n",
    "* https://www.youtube.com/watch?v=9mELEARcxJo\n",
    "* https://www.youtube.com/watch?v=U-rqJEKFzVE\n",
    "* https://porizi.wordpress.com/2014/02/21/flatmap-explained/\n",
    "* https://cosminpupaza.wordpress.com/2015/10/28/imperative-programming-vs-functional-programming-a-beginners-approach-part-1-map/\n",
    "* http://www.braveclojure.com/core-functions-in-depth/\n",
    "* https://www.youtube.com/watch?v=4ZH6mpIFbrY\n",
    "* https://www.youtube.com/watch?v=borv_KMI9Ac\n",
    "* https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext\n",
    "It's your window to the cluster. Allows you the following things\n",
    "* Create RDDs\n",
    "* Counters and Accumulators to communicate between nodes\n",
    "* It's automatically created on the pyspark shell (or notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "#sc = SparkContext( 'spark://headnodehost:7077', 'pyspark')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD\n",
    "Basically we distribute some data on the cluster. The idea is that latter we do some processing on it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Create a distributed list of numbers on the cluster\n",
    "nums = sc.parallelize(range(20))\n",
    "print(type(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361]\n"
     ]
    }
   ],
   "source": [
    "# Define a function that will be executed on each element\n",
    "# Will only be executed when we call collect \n",
    "squared = nums.map(lambda x:x*x)\n",
    "\n",
    "# Now execute the function\n",
    "print(squared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Sample: [0, 1, 5, 11, 13, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "print(nums.collect())\n",
    "print('Sample:',nums.sample(fraction=0.2, withReplacement=False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "reduce_nums_sum = nums.reduce(lambda x,y:x+y)\n",
    "print(reduce_nums_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Passing functions to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function will be distributed on the cluster to execute \n",
    "def someOperation(input):\n",
    "    return (input * 2) + 1\n",
    "\n",
    "# Apply the function \"someOperation\" on all elements of the RDD\n",
    "nums.map(someOperation).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Programming\n",
    "Most of the operations on apache spark comes from function programming paradigm\n",
    "\n",
    "#### Map\n",
    "The Map operation assign a function to each element of your data. It returns the same amount of data as the input.\n",
    "![Map](map.png)\n",
    "\n",
    "#### MapFlat\n",
    "Similar to map, but each input item can be mapped to 0 or more output items\n",
    "![MapFlat](flatmap.png)\n",
    "\n",
    "#### Reduce\n",
    "The Reduce continually applies the function to your data. It returns a single value\n",
    "![Reduce Diagram](reduce_diagram.png)\n",
    "\n",
    "#### Reduce By Key\n",
    "It works like reduce but it apply only on elements with same key. It returns \"key\" values \n",
    "![Reduce by Key Diagram](reduce_by_key.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leonardo', 'araujo', 'dos', 'santos', 'is', 'leonardo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('leonardo', 1),\n",
       " ('araujo', 1),\n",
       " ('dos', 1),\n",
       " ('santos', 1),\n",
       " ('is', 1),\n",
       " ('leonardo', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the string on spaces and store into a list of strings\n",
    "lst_words = 'leonardo araujo dos santos is leonardo'.split(\" \")\n",
    "print(lst_words)\n",
    "lst_words_with_count = map(lambda x: (x,1), lst_words)\n",
    "list(lst_words_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a text file and distribute it's lines across the cluster\n",
    "lines = sc.textFile('../data/TweetsText/tweets2.txt')\n",
    "\n",
    "# Get list of all words\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# Count words: The reduce by key will group your lambda to execute only on same values \n",
    "word_count = (words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y))\n",
    "\n",
    "# The computation actually starts here....\n",
    "word_count.saveAsTextFile('word_count_result')\n",
    "word_count.saveAsHadoopFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all lines that says something about Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The BBC's Washington correspondents track developments in the first 100 days of Barack Obama's presidency. http://tinyurl.com/d97jot\",\n",
       " \"Obama's Conversation Starter: National Journal.com's Amy Harder wrote an interesting profile of Joe Rospars who .. http://tinyurl.com/ck8436\",\n",
       " 'yuo refuse to accept the facts, only dismiss; Why would your pal Obama do everything he can with team of lawyers to block #TCOT',\n",
       " 'Its as ridiculous as you posing as #TCOT but defending the #Obama tyranny LOL #TCOT',\n",
       " 'Confirmed: The Obama DHS hit job on conservatives is real - Malkin - April 14 http://ff.im/-279qc',\n",
       " \"Now that Obama is president it puts severe limits on how he can use email and indeed Twitter. That's why his account is dead.\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the rdd using a defined function\n",
    "linesObama = lines.filter(lambda x: \"obama\" in x.lower())\n",
    "print(linesObama.count())\n",
    "\n",
    "# Just show the lines\n",
    "linesObama.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
